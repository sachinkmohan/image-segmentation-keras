{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b578bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.all_models import model_from_name\n",
    "import six\n",
    "from keras_segmentation.data_utils.data_loader import image_segmentation_generator, \\\n",
    "    verify_segmentation_dataset\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d28933",
   "metadata": {},
   "source": [
    "### Use the below if you are training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "\n",
    "model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.load_weights('divam_ss_base.03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b00353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, six.string_types):\n",
    "    # create the model from the name\n",
    "    assert (n_classes is not None), \"Please provide the n_classes\"\n",
    "    if (input_height is not None) and (input_width is not None):\n",
    "        model = model_from_name[model](\n",
    "            n_classes, input_height=input_height, input_width=input_width)\n",
    "    else:\n",
    "        model = model_from_name[model](n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.load_weights('divam_ss_base.03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = model.n_classes\n",
    "input_height = model.input_height\n",
    "input_width = model.input_width\n",
    "output_height = model.output_height\n",
    "output_width = model.output_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 50\n",
    "input_height = 320\n",
    "input_width = 640\n",
    "output_height = 160\n",
    "output_width = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_k = 'categorical_crossentropy'\n",
    "model.compile(loss=loss_k,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4a32f",
   "metadata": {},
   "source": [
    "### Execute this if you are planning to predict or train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=\"dataset1/images_prepped_train/\"\n",
    "train_annotations=\"dataset1/annotations_prepped_train/\"\n",
    "#input_height=None\n",
    "#input_width=None\n",
    "#n_classes=None\n",
    "verify_dataset=True\n",
    "checkpoints_path=None\n",
    "epochs=1\n",
    "batch_size=2\n",
    "validate=False\n",
    "val_images=None\n",
    "val_annotations=None\n",
    "val_batch_size=2\n",
    "auto_resume_checkpoint=False\n",
    "load_weights=None\n",
    "steps_per_epoch=512\n",
    "val_steps_per_epoch=512\n",
    "gen_use_multiprocessing=False\n",
    "ignore_zero_class=False\n",
    "optimizer_name='adam'\n",
    "do_augment=False\n",
    "augmentation_name=\"aug_all\",\n",
    "callbacks=None\n",
    "custom_augmentation=None\n",
    "other_inputs_paths=None\n",
    "preprocessing=None\n",
    "read_image_type=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = image_segmentation_generator(\n",
    "        train_images, train_annotations,  batch_size,  n_classes,\n",
    "        input_height, input_width, output_height, output_width,\n",
    "        do_augment=do_augment, augmentation_name=augmentation_name,\n",
    "        custom_augmentation=custom_augmentation, other_inputs_paths=other_inputs_paths,\n",
    "        preprocessing=preprocessing, read_image_type=read_image_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = 'divam_ss_base_test'\n",
    "default_callback = ModelCheckpoint(\n",
    "            filepath=checkpoints_path + \".{epoch:02d}.h5\",\n",
    "            save_weights_only=False,\n",
    "            verbose=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250735f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch=0\n",
    "model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=epochs, callbacks=default_callback, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dg_base_ep1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=model.load_model('dg_base_ep1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af352c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec952cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(image_input,\n",
    "                    width, height,\n",
    "                    imgNorm=\"sub_mean\", ordering='channels_first', read_image_type=1):\n",
    "    \"\"\" Load image array from input \"\"\"\n",
    "\n",
    "    if type(image_input) is np.ndarray:\n",
    "        # It is already an array, use it as it is\n",
    "        img = image_input\n",
    "    elif isinstance(image_input, six.string_types):\n",
    "        if not os.path.isfile(image_input):\n",
    "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
    "                                  .format(image_input))\n",
    "        img = cv2.imread(image_input, read_image_type)\n",
    "    else:\n",
    "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
    "                              .format(str(type(image_input))))\n",
    "\n",
    "    if imgNorm == \"sub_and_divide\":\n",
    "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
    "    elif imgNorm == \"sub_mean\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.atleast_3d(img)\n",
    "\n",
    "        means = [103.939, 116.779, 123.68]\n",
    "\n",
    "        for i in range(min(img.shape[2], len(means))):\n",
    "            img[:, :, i] -= means[i]\n",
    "\n",
    "        img = img[:, :, ::-1]\n",
    "    elif imgNorm == \"divide\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "\n",
    "    if ordering == 'channels_first':\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de160ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "IMAGE_ORDERING = \"channels_last\"\n",
    "x = get_image_array(inp, input_width, input_height,\n",
    "                    ordering=IMAGE_ORDERING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model7.predict(np.array([x]))[0]\n",
    "pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class_colors = [(random.randint(0, 255), random.randint(\n",
    "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
    "\n",
    "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
    "                           colors=class_colors, class_names=None,\n",
    "                           overlay_img=False, show_legends=False,\n",
    "                           prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(seg_arr)\n",
    "\n",
    "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
    "\n",
    "    if inp_img is not None:\n",
    "        original_h = inp_img.shape[0]\n",
    "        original_w = inp_img.shape[1]\n",
    "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if (prediction_height is not None) and (prediction_width is not None):\n",
    "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
    "        if inp_img is not None:\n",
    "            inp_img = cv2.resize(inp_img,\n",
    "                                 (prediction_width, prediction_height))\n",
    "\n",
    "    if overlay_img:\n",
    "        assert inp_img is not None\n",
    "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
    "\n",
    "    if show_legends:\n",
    "        assert class_names is not None\n",
    "        legend_img = get_legends(class_names, colors=colors)\n",
    "\n",
    "        seg_img = concat_lenends(seg_img, legend_img)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
    "    output_height = seg_arr.shape[0]\n",
    "    output_width = seg_arr.shape[1]\n",
    "\n",
    "    seg_img = np.zeros((output_height, output_width, 3))\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        seg_arr_c = seg_arr[:, :] == c\n",
    "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
    "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
    "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
    "\n",
    "    return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_img=False\n",
    "show_legends=False\n",
    "class_names=None\n",
    "prediction_width=None\n",
    "prediction_height=None\n",
    "\n",
    "seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                 colors=class_colors, overlay_img=overlay_img,\n",
    "                                 show_legends=show_legends,\n",
    "                                 class_names=class_names,\n",
    "                                 prediction_width=prediction_width,\n",
    "                                 prediction_height=prediction_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    cv2.imshow('im', seg_img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60084169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(seg_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9c11b",
   "metadata": {},
   "source": [
    "### Use the below if you are loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "#model7=load_model('./striped_divam_ss_pruned.01.h5')\n",
    "model8=load_model('./divam_ss_base_test.01.h5')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57be9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "inp = cv2.imread(\"dataset1/images_prepped_test/0016E5_07965.png\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b55d",
   "metadata": {},
   "source": [
    "### Pruning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a89b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot \n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule as pruning_sched\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ce848",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=1000)\n",
    "}\n",
    "\n",
    "#'''\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model6, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_checkpoints_path = 'divam_ss_pruned'\n",
    "p_checkpoint = ModelCheckpoint(filepath=p_checkpoints_path + \".{epoch:02d}.h5\",save_weights_only=False,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23528d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './pruning_logs'\n",
    "p_callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    # Log sparsity and other metrics in Tensorboard.\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir),p_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.compile(loss=loss_k,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch=0\n",
    "model_for_pruning.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=1, callbacks=p_callbacks, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model_for_pruning.predict(np.array([x]))[0]\n",
    "pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8c0bf",
   "metadata": {},
   "source": [
    "### Strip pruning to remove the pruning layer names to the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989154f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_export.save('striped_divam_ss_pruned.01.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d9359",
   "metadata": {},
   "source": [
    "### End to end inference from a loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d76e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras_segmentation.models.unet import vgg_unet\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "#model7=load_model('./striped_divam_ss_pruned.01.h5')\n",
    "model9=load_model('./divam_ss_base_test.01.h5')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6717ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "inp = cv2.imread(\"dataset1/images_prepped_test/0016E5_07965.png\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78453ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 50\n",
    "input_height = 320\n",
    "input_width = 640\n",
    "output_height = 160\n",
    "output_width = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef22174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(image_input,\n",
    "                    width, height,\n",
    "                    imgNorm=\"sub_mean\", ordering='channels_first', read_image_type=1):\n",
    "    \"\"\" Load image array from input \"\"\"\n",
    "\n",
    "    if type(image_input) is np.ndarray:\n",
    "        # It is already an array, use it as it is\n",
    "        img = image_input\n",
    "    elif isinstance(image_input, six.string_types):\n",
    "        if not os.path.isfile(image_input):\n",
    "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
    "                                  .format(image_input))\n",
    "        img = cv2.imread(image_input, read_image_type)\n",
    "    else:\n",
    "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
    "                              .format(str(type(image_input))))\n",
    "\n",
    "    if imgNorm == \"sub_and_divide\":\n",
    "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
    "    elif imgNorm == \"sub_mean\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.atleast_3d(img)\n",
    "\n",
    "        means = [103.939, 116.779, 123.68]\n",
    "\n",
    "        for i in range(min(img.shape[2], len(means))):\n",
    "            img[:, :, i] -= means[i]\n",
    "\n",
    "        img = img[:, :, ::-1]\n",
    "    elif imgNorm == \"divide\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "\n",
    "    if ordering == 'channels_first':\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c49df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "IMAGE_ORDERING = \"channels_last\"\n",
    "x = get_image_array(inp, input_width, input_height,\n",
    "                    ordering=IMAGE_ORDERING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model9.predict(np.array([x]))[0]\n",
    "pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2552a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class_colors = [(random.randint(0, 255), random.randint(\n",
    "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
    "\n",
    "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
    "                           colors=class_colors, class_names=None,\n",
    "                           overlay_img=False, show_legends=False,\n",
    "                           prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(seg_arr)\n",
    "\n",
    "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
    "\n",
    "    if inp_img is not None:\n",
    "        original_h = inp_img.shape[0]\n",
    "        original_w = inp_img.shape[1]\n",
    "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if (prediction_height is not None) and (prediction_width is not None):\n",
    "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
    "        if inp_img is not None:\n",
    "            inp_img = cv2.resize(inp_img,\n",
    "                                 (prediction_width, prediction_height))\n",
    "\n",
    "    if overlay_img:\n",
    "        assert inp_img is not None\n",
    "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
    "\n",
    "    if show_legends:\n",
    "        assert class_names is not None\n",
    "        legend_img = get_legends(class_names, colors=colors)\n",
    "\n",
    "        seg_img = concat_lenends(seg_img, legend_img)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
    "    output_height = seg_arr.shape[0]\n",
    "    output_width = seg_arr.shape[1]\n",
    "\n",
    "    seg_img = np.zeros((output_height, output_width, 3))\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        seg_arr_c = seg_arr[:, :] == c\n",
    "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
    "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
    "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
    "\n",
    "    return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73fa29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_img=False\n",
    "show_legends=False\n",
    "class_names=None\n",
    "prediction_width=None\n",
    "prediction_height=None\n",
    "\n",
    "seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                 colors=class_colors, overlay_img=overlay_img,\n",
    "                                 show_legends=show_legends,\n",
    "                                 class_names=class_names,\n",
    "                                 prediction_width=prediction_width,\n",
    "                                 prediction_height=prediction_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e87a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb148104898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3181b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
