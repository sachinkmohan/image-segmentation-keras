{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b578bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.all_models import model_from_name\n",
    "import six\n",
    "from keras_segmentation.data_utils.data_loader import image_segmentation_generator, \\\n",
    "    verify_segmentation_dataset\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "## loading tensorboard\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d28933",
   "metadata": {},
   "source": [
    "### End to end training of the sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "\n",
    "model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ab10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_images =  \"dataset1/images_prepped_train/\",\n",
    "    train_annotations = \"dataset1/annotations_prepped_train/\",\n",
    "    checkpoints_path = \"/tmp/vgg_unet_1\" , epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model1.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb84f6e",
   "metadata": {},
   "source": [
    "### Loading a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1397a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "model1 = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "model1.load_weights('divam_ss_base_weights_ep20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba563eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('vgg_unet_ep1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0e413",
   "metadata": {},
   "source": [
    "### Loading a pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9958131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "model_p = load_model('./custom_model_files/striped_divam_ss_pruned_ep_model_20_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b00353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, six.string_types):\n",
    "    # create the model from the name\n",
    "    assert (n_classes is not None), \"Please provide the n_classes\"\n",
    "    if (input_height is not None) and (input_width is not None):\n",
    "        model = model_from_name[model](\n",
    "            n_classes, input_height=input_height, input_width=input_width)\n",
    "    else:\n",
    "        model = model_from_name[model](n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.load_weights('divam_ss_base.03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = model.n_classes\n",
    "input_height = model.input_height\n",
    "input_width = model.input_width\n",
    "output_height = model.output_height\n",
    "output_width = model.output_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a2cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 50\n",
    "input_height = 320\n",
    "input_width = 640\n",
    "output_height = 160\n",
    "output_width = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_k = 'categorical_crossentropy'\n",
    "model.compile(loss=loss_k,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4a32f",
   "metadata": {},
   "source": [
    "### Execute this if you are planning to predict or train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cd6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=\"dataset1/images_prepped_train/\"\n",
    "train_annotations=\"dataset1/annotations_prepped_train/\"\n",
    "#input_height=None\n",
    "#input_width=None\n",
    "#n_classes=None\n",
    "verify_dataset=True\n",
    "checkpoints_path=None\n",
    "epochs=20\n",
    "batch_size=2\n",
    "validate=False\n",
    "val_images=None\n",
    "val_annotations=None\n",
    "val_batch_size=2\n",
    "auto_resume_checkpoint=False\n",
    "load_weights=None\n",
    "steps_per_epoch=512\n",
    "val_steps_per_epoch=512\n",
    "gen_use_multiprocessing=False\n",
    "ignore_zero_class=False\n",
    "optimizer_name='adam'\n",
    "do_augment=False\n",
    "augmentation_name=\"aug_all\",\n",
    "callbacks=None\n",
    "custom_augmentation=None\n",
    "other_inputs_paths=None\n",
    "preprocessing=None\n",
    "read_image_type=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196fca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = image_segmentation_generator(\n",
    "        train_images, train_annotations,  batch_size,  n_classes,\n",
    "        input_height, input_width, output_height, output_width,\n",
    "        do_augment=do_augment, augmentation_name=augmentation_name,\n",
    "        custom_augmentation=custom_augmentation, other_inputs_paths=other_inputs_paths,\n",
    "        preprocessing=preprocessing, read_image_type=read_image_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "\n",
    "checkpoints_path = 'divam_ss_base_weights'\n",
    "default_callback = ModelCheckpoint(\n",
    "            filepath=checkpoints_path + \".{epoch:02d}.h5\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=True,\n",
    "            monitor='val_loss'\n",
    "        )\n",
    "tbCallBack = tensorflow.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "logs_base_dir = \"./Graph\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "#%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250735f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch=0\n",
    "model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=epochs, callbacks=[default_callback,tbCallBack], initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f66232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('divam_ss_base_weights_ep20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir path_to_c/Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"/tmp/out.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dg_base_ep20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=model.load_model('dg_base_ep1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af352c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec952cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(image_input,\n",
    "                    width, height,\n",
    "                    imgNorm=\"sub_mean\", ordering='channels_first', read_image_type=1):\n",
    "    \"\"\" Load image array from input \"\"\"\n",
    "\n",
    "    if type(image_input) is np.ndarray:\n",
    "        # It is already an array, use it as it is\n",
    "        img = image_input\n",
    "    elif isinstance(image_input, six.string_types):\n",
    "        if not os.path.isfile(image_input):\n",
    "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
    "                                  .format(image_input))\n",
    "        img = cv2.imread(image_input, read_image_type)\n",
    "    else:\n",
    "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
    "                              .format(str(type(image_input))))\n",
    "\n",
    "    if imgNorm == \"sub_and_divide\":\n",
    "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
    "    elif imgNorm == \"sub_mean\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.atleast_3d(img)\n",
    "\n",
    "        means = [103.939, 116.779, 123.68]\n",
    "\n",
    "        for i in range(min(img.shape[2], len(means))):\n",
    "            img[:, :, i] -= means[i]\n",
    "\n",
    "        img = img[:, :, ::-1]\n",
    "    elif imgNorm == \"divide\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "\n",
    "    if ordering == 'channels_first':\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de160ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "IMAGE_ORDERING = \"channels_last\"\n",
    "x = get_image_array(inp, input_width, input_height,\n",
    "                    ordering=IMAGE_ORDERING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model7_p.predict(np.array([x]))[0]\n",
    "pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class_colors = [(random.randint(0, 255), random.randint(\n",
    "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
    "\n",
    "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
    "                           colors=class_colors, class_names=None,\n",
    "                           overlay_img=False, show_legends=False,\n",
    "                           prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(seg_arr)\n",
    "\n",
    "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
    "\n",
    "    if inp_img is not None:\n",
    "        original_h = inp_img.shape[0]\n",
    "        original_w = inp_img.shape[1]\n",
    "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if (prediction_height is not None) and (prediction_width is not None):\n",
    "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
    "        if inp_img is not None:\n",
    "            inp_img = cv2.resize(inp_img,\n",
    "                                 (prediction_width, prediction_height))\n",
    "\n",
    "    if overlay_img:\n",
    "        assert inp_img is not None\n",
    "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
    "\n",
    "    if show_legends:\n",
    "        assert class_names is not None\n",
    "        legend_img = get_legends(class_names, colors=colors)\n",
    "\n",
    "        seg_img = concat_lenends(seg_img, legend_img)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
    "    output_height = seg_arr.shape[0]\n",
    "    output_width = seg_arr.shape[1]\n",
    "\n",
    "    seg_img = np.zeros((output_height, output_width, 3))\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        seg_arr_c = seg_arr[:, :] == c\n",
    "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
    "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
    "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
    "\n",
    "    return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_img=False\n",
    "show_legends=False\n",
    "class_names=None\n",
    "prediction_width=None\n",
    "prediction_height=None\n",
    "\n",
    "seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                 colors=class_colors, overlay_img=overlay_img,\n",
    "                                 show_legends=show_legends,\n",
    "                                 class_names=class_names,\n",
    "                                 prediction_width=prediction_width,\n",
    "                                 prediction_height=prediction_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    cv2.imshow('im', seg_img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60084169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9c11b",
   "metadata": {},
   "source": [
    "### Use the below if you are loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_segmentation.models.unet import vgg_unet\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "model7_p=load_model('./striped_divam_ss_pruned_ep_20.h5')\n",
    "#model_base=load_model('./dg_base_ep1.h5')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997cdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57be9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "inp = cv2.imread(\"dataset1/images_prepped_test/0016E5_07965.png\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b55d",
   "metadata": {},
   "source": [
    "### Pruning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a89b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot \n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule as pruning_sched\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97ce848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/virtualenv/divamgupta_tf2/lib/python3.6/site-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "num_images = 367\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "#'''\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model1, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6250de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_checkpoints_path = 'divam_ss_pruned_weights_n'\n",
    "p_checkpoint = ModelCheckpoint(filepath=p_checkpoints_path + \"{epoch:02d}.h5\",save_weights_only=True,save_best_only=True,\n",
    "                               monitor='val_loss',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23528d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './pruning_logs'\n",
    "p_callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    # Log sparsity and other metrics in Tensorboard.\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir),p_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a439890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better Model callbacks\n",
    "log_dir = '.\\logs'\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch = 100000000, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch'),\n",
    "  tfmot.sparsity.keras.PruningSummaries(\n",
    "    log_dir, update_freq='epoch'\n",
    "  )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133d2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_k = 'categorical_crossentropy'\n",
    "\n",
    "model_for_pruning.compile(loss=loss_k,\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69fa0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  6/512 [..............................] - ETA: 1:41 - loss: 0.0869 - accuracy: 0.9675WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0353s vs `on_train_batch_begin` time: 0.0384s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0353s vs `on_train_batch_end` time: 0.0995s). Check your callbacks.\n",
      "512/512 [==============================] - 48s 80ms/step - loss: 0.1540 - accuracy: 0.9496\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 2/20\n",
      "512/512 [==============================] - 41s 80ms/step - loss: 0.1229 - accuracy: 0.9567\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/20\n",
      "512/512 [==============================] - 42s 81ms/step - loss: 0.1186 - accuracy: 0.9575\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.1279 - accuracy: 0.9546\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/20\n",
      "512/512 [==============================] - 43s 84ms/step - loss: 0.1097 - accuracy: 0.9603\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/20\n",
      "512/512 [==============================] - 43s 84ms/step - loss: 0.0918 - accuracy: 0.9662\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/20\n",
      "512/512 [==============================] - 43s 84ms/step - loss: 0.0809 - accuracy: 0.9700\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0780 - accuracy: 0.9709\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0751 - accuracy: 0.9719\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0714 - accuracy: 0.9732\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 11/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0692 - accuracy: 0.9739\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 12/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0669 - accuracy: 0.9747\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 13/20\n",
      "512/512 [==============================] - 42s 82ms/step - loss: 0.0649 - accuracy: 0.9754\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 14/20\n",
      "512/512 [==============================] - 42s 82ms/step - loss: 0.0631 - accuracy: 0.9761\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 15/20\n",
      "512/512 [==============================] - 42s 82ms/step - loss: 0.0618 - accuracy: 0.9765\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 16/20\n",
      "512/512 [==============================] - 42s 82ms/step - loss: 0.0620 - accuracy: 0.9764\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 17/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0620 - accuracy: 0.9764\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 18/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0610 - accuracy: 0.9767\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 19/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0603 - accuracy: 0.9770\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 20/20\n",
      "512/512 [==============================] - 42s 83ms/step - loss: 0.0583 - accuracy: 0.9777\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab40439748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_epoch=0\n",
    "model_for_pruning.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=epochs, callbacks=p_callbacks, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model_for_pruning.predict(np.array([x]))[0]\n",
    "pr_p1 = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8c0bf",
   "metadata": {},
   "source": [
    "### Strip pruning to remove the pruning layer names to the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "989154f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_export.save('striped_divam_ss_pruned_ep_model_20_new.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee72f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.save_weights('divam_ss_pruned_ep_weights_20_new.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d9359",
   "metadata": {},
   "source": [
    "### End to end inference from a loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d76e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras_segmentation.models.unet import vgg_unet\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#model9 = vgg_unet(n_classes=50 ,  input_height=320, input_width=640  )\n",
    "#model7=load_model('./striped_divam_ss_pruned_ep_20.h5')\n",
    "model9=load_model('./dg_base_ep1.h5')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6717ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "inp = cv2.imread(\"dataset1/images_prepped_test/0016E5_07965.png\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78453ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 50\n",
    "input_height = 320\n",
    "input_width = 640\n",
    "output_height = 160\n",
    "output_width = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef22174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(image_input,\n",
    "                    width, height,\n",
    "                    imgNorm=\"sub_mean\", ordering='channels_first', read_image_type=1):\n",
    "    \"\"\" Load image array from input \"\"\"\n",
    "\n",
    "    if type(image_input) is np.ndarray:\n",
    "        # It is already an array, use it as it is\n",
    "        img = image_input\n",
    "    elif isinstance(image_input, six.string_types):\n",
    "        if not os.path.isfile(image_input):\n",
    "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
    "                                  .format(image_input))\n",
    "        img = cv2.imread(image_input, read_image_type)\n",
    "    else:\n",
    "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
    "                              .format(str(type(image_input))))\n",
    "\n",
    "    if imgNorm == \"sub_and_divide\":\n",
    "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
    "    elif imgNorm == \"sub_mean\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.atleast_3d(img)\n",
    "\n",
    "        means = [103.939, 116.779, 123.68]\n",
    "\n",
    "        for i in range(min(img.shape[2], len(means))):\n",
    "            img[:, :, i] -= means[i]\n",
    "\n",
    "        img = img[:, :, ::-1]\n",
    "    elif imgNorm == \"divide\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "\n",
    "    if ordering == 'channels_first':\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c49df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "IMAGE_ORDERING = \"channels_last\"\n",
    "x = get_image_array(inp, input_width, input_height,\n",
    "                    ordering=IMAGE_ORDERING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec052f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model_for_pruning.predict(np.array([x]))[0]\n",
    "pr_p = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2552a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class_colors = [(random.randint(0, 255), random.randint(\n",
    "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
    "\n",
    "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
    "                           colors=class_colors, class_names=None,\n",
    "                           overlay_img=False, show_legends=False,\n",
    "                           prediction_width=None, prediction_height=None):\n",
    "\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(seg_arr)\n",
    "\n",
    "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
    "\n",
    "    if inp_img is not None:\n",
    "        original_h = inp_img.shape[0]\n",
    "        original_w = inp_img.shape[1]\n",
    "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    if (prediction_height is not None) and (prediction_width is not None):\n",
    "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
    "        if inp_img is not None:\n",
    "            inp_img = cv2.resize(inp_img,\n",
    "                                 (prediction_width, prediction_height))\n",
    "\n",
    "    if overlay_img:\n",
    "        assert inp_img is not None\n",
    "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
    "\n",
    "    if show_legends:\n",
    "        assert class_names is not None\n",
    "        legend_img = get_legends(class_names, colors=colors)\n",
    "\n",
    "        seg_img = concat_lenends(seg_img, legend_img)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
    "    output_height = seg_arr.shape[0]\n",
    "    output_width = seg_arr.shape[1]\n",
    "\n",
    "    seg_img = np.zeros((output_height, output_width, 3))\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        seg_arr_c = seg_arr[:, :] == c\n",
    "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
    "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
    "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
    "\n",
    "    return seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_img=False\n",
    "show_legends=False\n",
    "class_names=None\n",
    "prediction_width=None\n",
    "prediction_height=None\n",
    "\n",
    "seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                 colors=class_colors, overlay_img=overlay_img,\n",
    "                                 show_legends=show_legends,\n",
    "                                 class_names=class_names,\n",
    "                                 prediction_width=prediction_width,\n",
    "                                 prediction_height=prediction_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e87a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa84b2f2e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(pr_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1e3181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "class_colors = [(random.randint(0, 255), random.randint(\n",
    "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
    "\n",
    "\n",
    "def evaluate(model=None, inp_images=None, annotations=None,\n",
    "             inp_images_dir=None, annotations_dir=None, checkpoints_path=None, read_image_type=1):\n",
    "\n",
    "    if model is None:\n",
    "        assert (checkpoints_path is not None),\\\n",
    "                \"Please provide the model or the checkpoints_path\"\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    if inp_images is None:\n",
    "        assert (inp_images_dir is not None),\\\n",
    "                \"Please provide inp_images or inp_images_dir\"\n",
    "        assert (annotations_dir is not None),\\\n",
    "            \"Please provide inp_images or inp_images_dir\"\n",
    "\n",
    "        paths = get_pairs_from_paths(inp_images_dir, annotations_dir)\n",
    "        paths = list(zip(*paths))\n",
    "        inp_images = list(paths[0])\n",
    "        annotations = list(paths[1])\n",
    "\n",
    "    assert type(inp_images) is list\n",
    "    assert type(annotations) is list\n",
    "\n",
    "    tp = np.zeros(50)\n",
    "    fp = np.zeros(50)\n",
    "    fn = np.zeros(50)\n",
    "    n_pixels = np.zeros(50)\n",
    "\n",
    "    for inp, ann in tqdm(zip(inp_images, annotations)):\n",
    "        pr = predict(model, inp, read_image_type=read_image_type)\n",
    "        gt = get_segmentation_array(ann, model.n_classes,\n",
    "                                    model.output_width, model.output_height,\n",
    "                                    no_reshape=True, read_image_type=read_image_type)\n",
    "        gt = gt.argmax(-1)\n",
    "        pr = pr.flatten()\n",
    "        gt = gt.flatten()\n",
    "\n",
    "        for cl_i in range(model.n_classes):\n",
    "\n",
    "            tp[cl_i] += np.sum((pr == cl_i) * (gt == cl_i))\n",
    "            fp[cl_i] += np.sum((pr == cl_i) * ((gt != cl_i)))\n",
    "            fn[cl_i] += np.sum((pr != cl_i) * ((gt == cl_i)))\n",
    "            n_pixels[cl_i] += np.sum(gt == cl_i)\n",
    "\n",
    "    cl_wise_score = tp / (tp + fp + fn + 0.000000000001)\n",
    "    n_pixels_norm = n_pixels / np.sum(n_pixels)\n",
    "    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n",
    "    mean_IU = np.mean(cl_wise_score)\n",
    "\n",
    "    return {\n",
    "        \"frequency_weighted_IU\": frequency_weighted_IU,\n",
    "        \"mean_IU\": mean_IU,\n",
    "        \"class_wise_IU\": cl_wise_score\n",
    "    }\n",
    "\n",
    "\n",
    "def predict(model=None, inp=None, out_fname=None,\n",
    "            checkpoints_path=None, overlay_img=False,\n",
    "            class_names=None, show_legends=False, colors=class_colors,\n",
    "            prediction_width=None, prediction_height=None,\n",
    "            read_image_type=1):\n",
    "\n",
    "    if model is None and (checkpoints_path is not None):\n",
    "        model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    assert (inp is not None)\n",
    "    assert ((type(inp) is np.ndarray) or isinstance(inp, six.string_types)),\\\n",
    "        \"Input should be the CV image or the input file name\"\n",
    "\n",
    "    if isinstance(inp, six.string_types):\n",
    "        inp = cv2.imread(inp, read_image_type)\n",
    "\n",
    "    #assert (len(inp.shape) == 3 or len(inp.shape) == 1 or len(inp.shape) == 4), \"Image should be h,w,3 \"\n",
    "\n",
    "    n_classes = 50\n",
    "    input_height = 320\n",
    "    input_width = 640\n",
    "    output_height = 160\n",
    "    output_width = 320\n",
    "\n",
    "    x = get_image_array(inp, input_width, input_height,\n",
    "                        ordering=IMAGE_ORDERING)\n",
    "    pr = model.predict(np.array([x]))[0]\n",
    "    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n",
    "\n",
    "    seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
    "                                     colors=colors, overlay_img=overlay_img,\n",
    "                                     show_legends=show_legends,\n",
    "                                     class_names=class_names,\n",
    "                                     prediction_width=prediction_width,\n",
    "                                     prediction_height=prediction_height)\n",
    "\n",
    "    if out_fname is not None:\n",
    "        cv2.imwrite(out_fname, seg_img)\n",
    "\n",
    "    return pr\n",
    "\n",
    "class DataLoaderError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fdf47ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-dd16e0a31af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_annotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-7212c57ffcf8>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, inp_images, annotations, inp_images_dir, annotations_dir, checkpoints_path, read_image_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(evaluate(model_p, train_images, train_annotations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
